{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today is valentines data.', 'Today is the day when lovers make love.', 'Rest of the day they keep on fighting with each other but today they have to be cautious not to fuck up with their lover.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = 'Today is valentines data. Today is the day when lovers make love. Rest of the day they keep on fighting with each other but today they have to be cautious not to fuck up with their lover.'\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'is', 'valentines', 'data', '.', 'Today', 'is', 'the', 'day', 'when', 'lovers', 'make', 'love', '.', 'Rest', 'of', 'the', 'day', 'they', 'keep', 'on', 'fighting', 'with', 'each', 'other', 'but', 'today', 'they', 'have', 'to', 'be', 'cautious', 'not', 'to', 'fuck', 'up', 'with', 'their', 'lover', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'her', 'can', 'under', 'me', 'couldn', 'do', 'over', 'doesn', \"should've\", 'not', 'needn', 'but', 'there', 'herself', 'whom', 'you', 'should', 'am', 'are', 'have', 'during', 'if', \"didn't\", 'did', 'isn', 'has', 'ourselves', 'about', \"it's\", 'how', 'd', 'my', 'any', 'so', 'until', 'once', 'of', 'hers', 'few', 'don', 'each', \"that'll\", 'as', 'were', \"shan't\", \"shouldn't\", 'down', 'only', \"haven't\", 'below', 'out', \"don't\", 'didn', 'from', 'wouldn', \"aren't\", 'be', \"isn't\", 'against', \"weren't\", \"couldn't\", 'myself', 'just', \"you'd\", 'won', 'when', 'then', 'a', 've', 'while', 'other', 'same', \"you're\", 'ma', \"hasn't\", 'yours', 'is', 'here', 'both', 'y', 'those', 'they', 'own', 'off', 'now', \"wasn't\", 'shouldn', 'where', 'above', 'and', 'than', 'our', 'his', 'aren', 'it', 'between', 'we', 'will', 'm', 'with', 'more', \"doesn't\", \"you've\", 'some', 'wasn', 'weren', 'itself', 'i', \"mustn't\", 'all', 'doing', 'ain', 'by', 'such', 'before', 'its', 'she', 'what', 'who', 'or', 'll', \"wouldn't\", 'o', 'through', 'why', 'ours', 'an', 'very', 'in', 'shan', 'their', 'your', \"mightn't\", 'he', 'this', 'no', 'which', \"needn't\", 'mightn', 'further', 'to', 'himself', 'up', \"hadn't\", 'them', 'theirs', 'had', 'hasn', 'does', 'on', 'into', 'themselves', 'because', 'nor', 't', \"you'll\", 'again', 'at', \"won't\", 'too', 'mustn', 'that', 'been', 'was', 'most', 'being', 'yourselves', 'the', 'haven', 'for', 's', 'after', 'him', \"she's\", 'these', 'yourself', 'having', 'hadn', 're'}\n"
     ]
    }
   ],
   "source": [
    "#removing stop words \n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Love', 'in', 'todays', 'world', 'seems', 'to', 'be', 'a', 'bit', 'temporary', '.', 'In', 'fact', 'love', 'is', 'temporary', '.', 'If', 'you', 'become', 'love', 'instead', 'of', 'loving', 'someone', 'particular', ',', 'you', 'have', 'find', 'the']\n",
      "['Love', 'todays', 'world', 'seems', 'bit', 'temporary', '.', 'In', 'fact', 'love', 'temporary', '.', 'If', 'become', 'love', 'instead', 'loving', 'someone', 'particular', ',', 'find']\n",
      "['Love', 'todays', 'world', 'seems', 'bit', 'temporary', '.', 'In', 'fact', 'love', 'temporary', '.', 'If', 'become', 'love', 'instead', 'loving', 'someone', 'particular', ',', 'find']\n"
     ]
    }
   ],
   "source": [
    "example =  'Love in todays world seems to be a bit temporary. In fact love is temporary. If you become love instead of loving someone particular, you have find the'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(word_tokens)\n",
    "print(sentence)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "player\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "# Stemming words with NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['play', 'playing', 'player','play']\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payer\n",
      "while\n",
      "pay\n",
      "plan\n",
      "about\n",
      "play\n",
      "to\n",
      "play\n",
      "their\n",
      "next\n",
      "play\n",
      "and\n",
      "which\n",
      "palyer\n",
      "will\n",
      "be\n",
      "play\n",
      "given\n",
      "play\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stemming an entire sentence\n",
    "\n",
    "new_text = 'payer while paying plan about playing to play their next play and which palyer will be playing given play.'\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "print(udhr.raw('English-Latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can train the PunktSentenceTokenizer using the text we have.\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets tokenize the sample text\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Define a functin that will tag each tokenized word with a part fo speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
      "(Chunk Capitol/NNP dome/NN)\n",
      "(Chunk have/VBP served/VBN America/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Union/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n"
     ]
    }
   ],
   "source": [
    "#Chunking in NLTK\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            \n",
    "            # combine the part of speech tag with a regualr expressiton\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # print the nltk tree\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                  print(subtree)\n",
    "            \n",
    "            #draw the chunks with nltk\n",
    "            #chunked.draw()\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinking with NLTK\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # The main difference here is the }{ vs {}.\n",
    "            # combine the part of speech tag with a regualr expressiton\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                }<VB.?|IN|DT|TO>{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # print the nltk tree\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                  print(subtree)\n",
    "            \n",
    "            #draw the chunks with nltk\n",
    "            #chunked.draw()\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #draw the chunks with nltk\n",
    "            namedEnt.draw()\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 2000\n",
      "First Review: (['there', 'was', 'a', 'time', 'when', 'john', 'carpenter', 'was', 'a', 'great', 'horror', 'director', '.', 'of', 'course', ',', 'his', 'best', 'film', 'was', '1978', \"'\", 's', 'masterpiece', ',', '\"', 'halloween', ',', '\"', 'but', 'he', 'also', 'made', '1980', \"'\", 's', '\"', 'the', 'fog', ',', '\"', 'and', '1987', \"'\", 's', 'underrated', ',', '\"', 'prince', 'of', 'darkness', '.', '\"', 'heck', ',', 'he', 'even', 'made', 'a', 'good', 'film', 'in', '1995', ',', 'with', '\"', 'in', 'the', 'mouth', 'of', 'madness', '.', '\"', 'but', 'something', 'terribly', 'wrong', 'happened', 'to', 'him', 'in', '1992', ',', 'with', 'the', 'terrible', 'comedy', ',', '\"', 'memoirs', 'of', 'an', 'invisible', 'man', '.', '\"', 'somehow', ',', 'carpenter', 'has', 'lost', 'his', 'touch', ',', 'with', 'junk', 'like', 'his', 'failed', '1995', 'remake', 'of', ',', '\"', 'village', 'of', 'the', 'damned', ',', '\"', 'to', 'his', 'uninspired', '1996', 'sequel', ',', '\"', 'escape', 'from', 'l', '.', 'a', '.', '\"', 'those', 'movies', ',', 'however', ',', 'look', 'like', 'cinematic', 'works', 'of', 'art', 'compared', 'to', 'his', 'latest', 'film', ',', '\"', 'john', 'carpenter', \"'\", 's', 'vampires', '.', '\"', 'if', 'i', 'was', 'him', ',', 'i', 'definately', 'wouldn', \"'\", 't', 'want', 'to', 'put', 'my', 'own', 'name', 'in', 'the', 'title', '.', 'it', 'is', 'a', 'sad', 'state', 'of', 'affairs', 'when', 'carpenter', 'can', 'make', 'something', 'as', 'misguided', 'and', 'flatly', 'written', 'and', 'filmed', 'as', ',', '\"', 'vampires', '.', '\"', 'the', 'story', 'is', 'simple', '.', 'jack', 'crow', '(', 'james', 'woods', ')', 'is', 'a', 'vampire', 'hunter', 'who', ',', 'along', 'with', 'one', 'of', 'his', 'partners', ',', 'montoya', '(', 'daniel', 'baldwin', ')', ',', 'and', 'a', 'prostitute', ',', 'katrina', '(', 'sheryl', 'lee', ')', ',', 'survives', 'an', 'attack', 'from', 'the', 'master', 'vampire', ',', 'valek', '(', 'thomas', 'ian', 'griffith', ')', '.', 'since', 'katrina', 'was', 'previously', 'bitten', 'by', 'him', ',', 'crow', 'takes', 'her', 'along', 'because', 'anyone', 'who', 'is', 'bitten', 'by', 'valek', 'becomes', 'telepathically', 'linked', 'to', 'him', 'until', 'they', 'themselves', 'turn', 'into', 'vampires', 'a', 'couple', 'days', 'later', ',', 'and', 'crow', 'is', 'hoping', 'to', 'find', 'him', 'with', 'the', 'help', 'of', 'her', '.', 'it', 'seems', 'valek', \"'\", 's', 'mission', 'is', 'to', 'steal', 'a', 'black', ',', 'wooden', 'cross', 'from', 'a', 'roman', 'catholic', 'church', 'that', 'will', 'enable', 'him', 'to', 'become', 'so', 'powerful', 'that', 'sunlight', 'will', 'not', 'destroy', 'him', '.', 'my', 'question', 'is', ':', 'how', 'many', 'time', 'have', 'we', 'seen', 'this', 'same', 'story', 'played', 'out', '?', 'well', ',', 'the', 'answer', 'is', 'just', 'about', 'as', 'many', 'times', 'as', 'a', 'better', 'version', 'of', 'the', 'story', 'has', 'been', 'made', '.', '\"', 'john', 'carpenter', \"'\", 's', 'vampires', ',', '\"', 'sadly', 'enough', ',', 'is', 'one', 'of', 'the', 'most', 'unscary', 'horror', 'films', 'i', \"'\", 've', 'ever', 'seen', '.', 'in', 'fact', ',', 'there', 'isn', \"'\", 't', 'even', 'one', 'suspenseful', 'moment', 'in', 'the', 'whole', '105', '-', 'minute', 'running', 'time', '.', 'the', 'non', '-', 'stop', 'vampire', 'attack', 'sequences', 'are', 'stylelessly', 'filmed', ',', 'without', 'any', 'interesting', 'camera', 'work', ',', 'which', 'is', 'usually', 'a', 'trademark', 'of', 'carpenter', \"'\", 's', '.', 'and', 'then', 'we', 'come', 'to', 'the', 'screenplay', ',', 'which', ',', 'as', 'far', 'as', 'i', 'can', 'tell', ',', 'is', 'nearly', 'non', '-', 'existent', '.', 'there', 'is', 'no', 'story', 'development', ',', 'and', 'there', 'isn', \"'\", 't', 'even', 'an', 'attempt', 'to', 'flesh', 'out', 'the', 'characters', '.', 'james', 'woods', 'can', 'be', 'a', 'good', 'actor', ',', 'but', 'he', 'has', 'nothing', 'to', 'do', 'here', 'but', 'to', 'say', 'a', 'couple', 'of', '\"', 'pseudo', '\"', '-', 'clever', 'lines', 'of', 'dialogue', '.', 'daniel', 'baldwin', 'has', 'some', 'potential', ',', 'but', 'his', 'character', 'comes', 'off', 'as', 'being', 'very', 'dense', '.', 'and', 'sheryl', 'lee', '(', 'faring', 'much', 'better', 'as', 'laura', 'palmer', 'in', '\"', 'twin', 'peaks', '\"', ')', ',', 'like', 'all', 'of', 'the', 'female', 'characters', ',', 'plays', 'an', 'offensive', 'stereotypical', 'whore', '.', 'there', 'is', 'not', 'an', 'ounce', 'of', 'intelligence', ',', 'or', 'excitement', 'in', ',', '\"', 'john', 'carpenter', \"'\", 's', 'vamires', ',', '\"', 'which', 'is', 'very', 'disheartening', 'coming', 'from', 'an', 'ex', '-', 'fan', 'of', 'carpenter', \"'\", 's', '.', 'he', 'has', 'said', 'that', 'he', 'turned', 'down', 'directing', ',', '\"', 'halloween', ':', 'h20', ',', '\"', 'because', 'he', 'couldn', \"'\", 't', 'work', 'up', 'any', 'excitement', 'for', 'it', '.', 'and', 'yet', ',', 'when', 'asked', 'about', 'a', '\"', 'vampires', '\"', 'sequel', ',', 'he', 'said', 'he', 'would', 'be', 'happy', 'to', 'do', 'it', '.', 'i', 'think', 'that', \"'\", 's', 'a', 'definite', 'sign', 'that', 'carpenter', 'has', 'finally', 'lost', 'any', 'trace', 'of', 'his', 'lasting', 'talent', ',', 'not', 'to', 'mention', 'a', 'significant', 'number', 'of', 'iq', 'points', '.'], 'neg')\n",
      "Most common words: [(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "The word happy: 215\n"
     ]
    }
   ],
   "source": [
    "# build list of documents\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "        \n",
    "# shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents: {}'.format(len(documents)))\n",
    "print('First Review: {}'.format(documents[0]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))\n",
    "print('The word happy: {}'.format(all_words[\"happy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 4000 most common words as features\n",
    "word_features  = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a find_features function that will determine which of the 400 word features are containted in a review\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "# lets use and example from a negative review\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do it for all the documents\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use sklearn algoriths in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SklearnClassifier(SVC(kernel = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on the training data\n",
    "model.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy : 0.812\n"
     ]
    }
   ],
   "source": [
    "# test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)\n",
    "print(\"SVC accuracy : {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
